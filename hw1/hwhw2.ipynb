{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mnist = np.loadtxt('./mnist.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(csv_dataset): # i는 0~100 중의 하나의 수로 train_set의 비율을 나타낸다. ex) 70 => train_set 70% test_set 30%\n",
    "# numpy matrix는 A[start:end:step] 형태로 표현이 가능하며, 다차원 배열의 원소 중 일부분만 접근하고 싶은 경우 파이썬 슬라이싱(slicing)과 comma(,)로 표현한다.\n",
    "#먼저 label과 data(픽셀)부분을 나눈다. label은 첫번째 열의 리스트이고, 나머지 열은 data(픽셀)에 해당하므로 위의 기준으로 slicing한다. 이 때, data는 정규화를 위해 256으로 나눈다.\n",
    "\n",
    "#그 후, 트레이닝 부분과 test부분으로 나눈다.  mnist.csv는 총 10000개의 행을 가지고 있고 0~7999는 training, 8000~9999는 test용으로 사용한다. 이 둘을 나누기 위해 위에서 구한 data와 label matrix를 8000번째 행을 기준으로 나눠 각각 저장한다.\n",
    "\n",
    "    label = csv_dataset[:, :1]\n",
    "    data = csv_dataset[:, 1:] /256\n",
    "    \n",
    "    train_X = data[:8000, :]\n",
    "    train_T = label[:8000, :]\n",
    "    test_X = data[8000: , ]\n",
    "    test_T = label[8000:, ]\n",
    "    \n",
    "    return train_X, train_T, test_X, test_T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(T): # T is data의 label\n",
    "# T는 2차원 배열로, 행은 input data의 개수이고 열은 label인 (8000,1)의 형태를 가진다.\n",
    "# one hot label은 data 개수만큼의 행을 가지고 class개수만큼의 열을 가진 2차원 array 형태로 표현한다. 행의 개수는 T.shape[0], 열의 개수는 10인 matrix이고 처음엔 모든 원소 값을 0으로 초기화한다.\n",
    "# 그 후 i번째 행에서 label 열만 0을 1로 바꾼다. label 열은  int(T[i])의 형태로 T의 i번째 행의값을 정수로 변환하여 접근한다.\n",
    "    data = np.zeros((T.shape[0],10))\n",
    "    for i in range (T.shape[0]):\n",
    "        data[i][int(T[i])]=1\n",
    "        i += 1\n",
    "    return data #one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Softmax(ScoreMatrix): # 제공.\n",
    "\n",
    "    if ScoreMatrix.ndim == 2:\n",
    "        temp = ScoreMatrix\n",
    "        temp = temp - np.max(temp, axis=1, keepdims=True)\n",
    "        y_predict = np.exp(temp) / np.sum(np.exp(temp), axis=1, keepdims=True)\n",
    "        return y_predict\n",
    "    temp = ScoreMatrix - np.max(ScoreMatrix, axis=0)\n",
    "    expX = np.exp(temp)\n",
    "    y_predict = expX / np.sum(expX)\n",
    "    return y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setParam_He(neuronlist):\n",
    "    \n",
    "    np.random.seed(1) # seed값 고정을 통해 input이 같으면 언제나 같은 Weight와 bias를 출력하기 위한 함수\n",
    "# neuronlist의 입력은 각 레이어의 뉴런 개수,[input Layer neuron, hidden layer1 neuron, hidden layer2 neuron, output layer neuron]으로 들어온다.\n",
    "# W1, b1은 input layer를 받아 forward함수를 사용할 때 쓰이는 weight parameter와 bias 값이며, W2, b2는 hidden layer1을 받아 forward 함수를 사용할 때 쓰이는 weight parameter와 bias값, 마지막으로 W3,b3은 hidden layer2를 받을 때 사용하는 값이다.\n",
    "# He 방식의 초기화는  W = np.random.randn(fan_in,fan_out) / np.sqrt(fan_in/2) 이므로 W의 fan_in에는 각각의 input, fan_out에는 각각의 output을 대입한다.\n",
    "#bias의 형태는 X*W와 같은 형태이어야 한다. 즉, b1은 X*W1와 같은 형태이며 이는 neuronlist[1]과 같다. 이와 마찬가지로 b2, b3의 형태를 구하면 된다.  \n",
    "    W1 = np.random.randn(neuronlist[0], neuronlist[1]) / np.sqrt(neuronlist[0]/2) \n",
    "    W2 = np.random.randn(neuronlist[1], neuronlist[2]) / np.sqrt(neuronlist[1]/2)\n",
    "    W3 = np.random.randn(neuronlist[2], neuronlist[3]) / np.sqrt(neuronlist[2]/2)\n",
    "    b1 = np.zeros(neuronlist[1])\n",
    "    b2 = np.zeros(neuronlist[2])\n",
    "    b3 = np.zeros(neuronlist[3])\n",
    "    \n",
    "    return W1, W2, W3, b1, b2, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linearLayer:\n",
    "    def __init__(self, W, b):\n",
    "        #backward에 필요한 X, W, b 값 저장 + dW, db값 받아오기\n",
    "        \n",
    "        self.X = None\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.X = x\n",
    "        #내적연산을 통한 Z값 계산\n",
    "        Z = np.dot(self.X, self.W) + self.b \n",
    "        return Z\n",
    "    #linear score function f(X,W) = X*W + b 이다. np.dot함수를 이용하여 X와 W행렬을 곱하고 마지막으로 bias값을 더하여 score값을 구하였다. \n",
    "   \n",
    "    def backward(self, dZ):\n",
    "        #백워드 함수\n",
    "        #backward 함수를 이용하여 최종적으로는 input인 x의 변화량에 따른 output Z의 값 변화량인 dx( dZ/dx)를 구하려 한다. parameter로 dZ가 주어지므로 dx = dZ*W.T 이고 dx와 더불어 dW값과 db값을 update한다.\n",
    "        #참고로, dW와 db는 각각 dZ/dW, dZ/db를 의미하는 것이다. \n",
    "        dx = np.dot(dZ,self.W.T)\n",
    "        self.dW = np.dot(self.X.T,dZ )\n",
    "        self.db = np.sum(dZ,axis = 0 )\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiLU:\n",
    "    def __init__(self):\n",
    "        self.Z = None # 백워드 시 사용할 로컬 변수\n",
    "        self.sig = None #백워드 시 사용할 로컬 변수\n",
    "    \n",
    "    def forward(self, Z):\n",
    "        #수식에 따른 forward 함수 작성\n",
    "        # linear function의 결과값인 Z에 activation function인 SiLU함수를 사용한다. SiLU함수는 x∗sigmoid(x)이다. 즉, 리턴값인 Activation은 앞에서 언급한 식의 x에 Z를 대입한 값이다.\n",
    "        sig = 1 / (1 + np.exp(-Z))\n",
    "        Activation = Z * sig\n",
    "        self.Z = Z\n",
    "        self.sig = sig\n",
    "        return Activation\n",
    "    \n",
    "    def backward(self, dActivation):\n",
    "        #input이 Z이고, activation 함수에 의해 만들어진 결과값이 out이라 하면, backward의 최종 결과값은 input의 변화량에 따른 output의 변화량이다. 즉, dx = dout*(activation함수의 미분값)이 리턴 값이다.\n",
    "        #SILU함수를 f(x)라 하고, x에 대한 sigmoid함수의 결과값을 sig라 하면 f(x) = sig *x, f'(x) = sig + x*(1-sig)*sig이다. 여기에 dActivate(Activate가 forward에서의 out이므로 의미상으로 dout에 해당한다.)를 곱한 값이 backward에서 최종적으로 리턴시키는 값인 input에 대한 미분값 dZ이다.\n",
    "        dZ = dActivation*(self.sig + self.Z*(1-self.sig)*self.sig)  \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss(): # 제공\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.softmaxScore = None\n",
    "        self.label = None\n",
    "        \n",
    "    def forward(self, score, one_hot_label):\n",
    "        \n",
    "        batch_size = one_hot_label.shape[0]\n",
    "        self.label = one_hot_label\n",
    "        self.softmaxScore = Softmax(score)\n",
    "        self.loss = -np.sum(self.label * np.log(self.softmaxScore + 1e-20)) / batch_size\n",
    "        \n",
    "        return self.loss\n",
    "    \n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.label.shape[0]\n",
    "        dx = (self.softmaxScore - self.label) / batch_size\n",
    "        \n",
    "        return dx\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dropOut : \n",
    "#만약 train_flg가 TRUE인 경우, dropOu의 forward를 진행하며  x와 같은 형상으로 배열을 random하게 생성하고, 그 값이 drop_outㅁratio보다 큰 원소를 TRUE로 설정하여 self.mask에 저장한다. 해당 mask를 기존의 input인 x에 곱한다. 그 결과 dropout_ratio만큼의 뉴런만 존재하게 된다. 만약 drop_out을 실행하고 싶지 않거나 drop_out실행 후 다시 원래의 상태로 돌아가려 한다면 train_flg를 FALSE로 실행하여 1 - self.dropout_ratio(삭제한 비율)를 x에 곱하여 리턴한다.\n",
    "#backward는 parameter로 out을 미분한 dout에 이미 저장되어있는 self.mask만크의 비율을 곱하여 리턴한다.\n",
    "#마지막으로 killRate함수를 이용하여 외부에서 ㄴ주어진 dropout_ratio를 self.dropout_ratio에 삽입한다.\n",
    "    def __init__(self, dropout_ratio = 0) :\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "    \n",
    "    def forward(self, x, train_flg = True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "        \n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask\n",
    "    def killRate(self,dropout_ratio) :\n",
    "        self.dropout_ratio = dropout_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerNet :\n",
    " #dropout을 수행할 경우를 위해 L1과 SiLU함수 사이, L2와 SiLU함수 사이에 각각 droOut 클래스 객체를 선언하였다. 또한 dropout을 사용하였을 떄의 foward를 진행하여 최종적으로 Loss를 구하는 함수인 forwardWithDropout함수를 선언하였다.\n",
    "\n",
    "    def __init__(self, paramlist):\n",
    "        \n",
    "        W1, W2, W3, b1, b2, b3 = setParam_He(paramlist)\n",
    "        self.flag = 0\n",
    "        self.params = {}\n",
    "        self.params['W1'] = W1\n",
    "        self.params['W2'] = W2\n",
    "        self.params['W3'] = W3\n",
    "        self.params['b1'] = b1\n",
    "        self.params['b2'] = b2\n",
    "        self.params['b3'] = b3\n",
    "        \n",
    "\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['L1'] = linearLayer(self.params['W1'], self.params['b1'])\n",
    "        self.dropOut1 = dropOut()\n",
    "        self.layers['SiLU1'] = SiLU()\n",
    "        self.layers['L2'] = linearLayer(self.params['W2'], self.params['b2'])\n",
    "        self.dropOut2 = dropOut()\n",
    "        self.layers['SiLU2'] = SiLU()\n",
    "        self.layers['L3'] = linearLayer(self.params['W3'], self.params['b3'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def scoreFunction(self, x):      \n",
    "        for layer in self.layers.values():\n",
    "            # 한 줄이 best \n",
    "            #과제2 document를 참고하면 각 layer에서 forward함수를 호출하여 마지막 레이어 전까지의 값을 구할 수 있었다.\n",
    "            #즉, linear layer와 SiLU 레이어 둘 다 forward함수를 호출할 수 있으므로 self.layer.values를 이용해 dictionary의 값만큼  x값에 지속적으로 layer.forward를 실행하면 activation function 직전까지의 score값을 알 수 있다.\n",
    "            x = layer.forward(x)     \n",
    "        score = x\n",
    "        return score\n",
    "    \n",
    "    def forwardWithDropout(self, x, label):\n",
    "    #dropout용 forward를 구하여 최종 loss값을 리턴하는 함수이다. forward 함수를 이용하여 loss를 구해가는 방향인 것은 맞지만, 히든 레이어에서 일정한 비율로 뉴런을 삭제 후 그 뉴런을 바탕으로 forward를 진행하는 forwardWithDropout함수를 기존의 forward대신 써야 하는 때가 있기 때문에 , 그 부분을 if문으로 처리하였다.\n",
    "       \n",
    "        for layer in self.layers:\n",
    "            x = self.layers[layer].forward(x)\n",
    "            if layer == 'L1': #layer가 L1인 경우,  다음 for문에서는 SiLU1의 forward가 호출될 것이다.그 전에 dropout1의 forward를 진행해야 한다. \n",
    "                x = self.dropOut1.forward(x)\n",
    "            if layer == 'L2': #layer가 L2인 경우, 다음 for문에서는 SiLU2의 forward가 호출될 것이다. 그 전에 dropout2의 forward를 진행한다. \n",
    "                x = self.dropOut2.forward(x)\n",
    "\n",
    "        return self.lastLayer.forward(x, label)  \n",
    "    \n",
    "    def killRate(self, kill_n_h1, kill_n_h2):\n",
    "        self.flag = 1\n",
    "        self.dropOut1.killRate(kill_n_h1)\n",
    "        self.dropOut2.killRate(kill_n_h2)\n",
    "        \n",
    "    def forward(self, x, label):\n",
    "        if(self.flag == 1):\n",
    "            return self.forwardWithDropout(x,label)\n",
    "        else:\n",
    "            score = self.scoreFunction(x)\n",
    "            return self.lastLayer.forward(score, label)\n",
    "    \n",
    "    def accuracy(self, x, label):\n",
    "        \n",
    "        score = self.scoreFunction(x)\n",
    "        score_argmax = np.argmax(score, axis=1)\n",
    "        \n",
    "        if label.ndim != 1 : #label이 one_hot_encoding 된 데이터면 if문을 \n",
    "            label_argmax = np.argmax(label, axis = 1)\n",
    "            \n",
    "        accuracy  = np.sum(score_argmax==label_argmax) / int(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    " \n",
    "    def backpropagation(self, x, label):\n",
    "        \n",
    "        #백워드 함수 작성 스코어펑션을 참고하세요 lastlayer는 ordered dictionary가 아니기 때문에 가장 먼저 호출 후 그 값을 변수로 저장한다.\n",
    "        #그 이후 orderd diction의 파이썬 문법인 reversed함수를 구현하여 forward 순서의 반대 순서로 접근이 가능하다. scorefunction와 같은 방식으로 forward 대신 backward를 실행하면 그대로 backward 연산하면 backpropagation을 구현할 수있다.\n",
    "        #backward 함수를 실행하면 dx값 뿐만 아니라 dW와 db의 값도 얻을 수 있으므로 해당 값들을 최종적으로 grad배열의 dW,db에 update시킨다.\n",
    "        backx = self.lastLayer.backward()\n",
    "        for layer in reversed(self.layers.values()):\n",
    "            backx = layer.backward(backx)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['L1'].dW\n",
    "        grads['b1'] = self.layers['L1'].db\n",
    "        grads['W2'] = self.layers['L2'].dW\n",
    "        grads['b2'] = self.layers['L2'].db\n",
    "        grads['W3'] = self.layers['L3'].dW\n",
    "        grads['b3'] = self.layers['L3'].db\n",
    "        \n",
    "        return grads\n",
    "    \n",
    "    def gradientdescent(self, grads, learning_rate):\n",
    "        \n",
    "        self.params['W1'] -= learning_rate*grads['W1']\n",
    "        self.params['W2'] -= learning_rate*grads['W2']\n",
    "        self.params['W3'] -= learning_rate*grads['W3']\n",
    "        self.params['b1'] -= learning_rate*grads['b1']\n",
    "        self.params['b2'] -= learning_rate*grads['b2']\n",
    "        self.params['b3'] -= learning_rate*grads['b3']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def batchOptimization(dataset, ThreeLayerNet, learning_rate, epoch=1000):\n",
    "  #epoch가 10번째일때마다 accuracy를 체크한다.위 함수에서는 아직 train_acc_list와 test_acc_list,Loss_list가 선언이 되어있지 않았으므로 선언을 해야 한다. 위의 세 리스트를 선언 후 accuracy에 저장한다. 빈 리스트의 선언은 '리스트명' = []로 선언하고 원소 추가는 append를 사용한다.     \n",
    "    train_acc_list = [] \n",
    "    test_acc_list = []\n",
    "    Loss_list = [] \n",
    "    \n",
    "    for i in range(epoch+1):\n",
    "\n",
    "#ThreeLayerNet의 forward함수는  마지막 layer 전까지 score를 구한 후 activation function을 통해 입력으로 들어온 data에 대한 Loss값을 구한다. 이를 이용하여 data_train과 data_test의 Loss를 구한다. \n",
    "\n",
    "#이제 loss값을 backpropergation을 하여 loss값이 줄어드는 방향으로 W와 b값을 최적화 해야 한다. backpropagation함수 역시threeLayerNet에 구현되어있으므로 해당 함수를 불러올 수 있다. backpropagation함수의 결과로 세 레이어 각각의 dW와 db값을 저장한다. 그런데 이 값을 그대로 update하는 것이 아니라 learning rate만큼 곱한 정도만큼을 W와 b에 update해야 하므로 gradientdescent 함수를 불러 learning rate를 곱한 정도만큼 값을 update한다. update된 값들은 self.params에 저장이 되며 이 과정까지가 한 epoch당 optimization하는 것이 된다. 이를 epoch만큼 실행하며, 10번마다 train_acc_list, test_acc_list,Loss_list에 저장 후 출력한다. 최종 리턴값은 언급한 list에 Threelayernet을 포함한다.\n",
    "\n",
    "#한편, dictionary 객체는 순서가 없기 때문에 key값으로 접근할 수 있으며,  key값 접근은 ' ' 형식이므로 dataset의 train_data와 one_hot_train(label을 one-hot-encoding형식으로 바꾼 것)를 dataset['train_data'], dataset['one_hot_train']형식으로 접근한다. 다른 data들 역시 위의 방식으로 접근한다. 따라서 함수를 구현하면 다음과 같은 방식으로 구현이 가능하다. \n",
    "        Loss = ThreeLayerNet.forward(dataset['train_X'], dataset['one_hot_train'])\n",
    "        back = ThreeLayerNet.backpropagation(dataset['train_X'], dataset['one_hot_train'])\n",
    "        ThreeLayerNet.gradientdescent(back, learning_rate)\n",
    "            \n",
    "        if (i % 10 == 0) and ((i//10) < 3 or ( i//10) >97):\n",
    "            train_acc = ThreeLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'])\n",
    "            test_acc = ThreeLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'])\n",
    "            print(i, '\\t번째 Loss = ', Loss)\n",
    "            print(i, '\\t번째 Train_Accuracy : ', train_acc)\n",
    "            print(i, '\\t번째 Test_Accuracy : ', test_acc)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            Loss_list.append(Loss)        \n",
    "   \n",
    "    return ThreeLayerNet, train_acc_list, test_acc_list, Loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_Optimization(dataset, ThreeLayerNet, learning_rate, epoch=100, batch_size=100):    \n",
    "    \n",
    "    np.random.seed(5)\n",
    "    for i in range(epoch+1):\n",
    "        # 코드 작성\n",
    "        #train_X와 one_hot_train을 np.random.shuffle을 사용해서 섞는다. 대신 한 쌍 단위로 섞어 한 쌍의 단위처럼 셔플이 되어야 한다. \n",
    "        #마찬가지로 train_acc_list, test_acc_list,Loss_list가 선언되어있지 않으므로 선언을 한다.\n",
    "        train_X = dataset['train_X']\n",
    "        train_T = dataset['one_hot_train']\n",
    "       # train_acc_list = []\n",
    "        #test_acc_list = []\n",
    "        #Loss_list = []  \n",
    "       # set = list(zip(train_X,train_T))\n",
    "        #np.random.shuffle(set)\n",
    "        #train_X,train_T = zip(*set)\n",
    "        \n",
    "        #shuffle된 data를 batch단위로 나눠서 forward와 backpropagation, gradientdescent를 한다. \n",
    "        #numpy.arange(Start,stop,step)함수를 사용하여 data를 batch_size만큼의 사이즈로 슬라이싱할 것이다. train_X의 행의 개수가 전체 data를 의미 하므로, 0부터 train_X.shape[0]을 이용한다. 자르는 단위는 batch_size(100)만큼 나눈다. 이 결과로 miniSize에는 minibatch로 나누어질 수 있는 배열의 개수가 나온다.\n",
    "        #for문을 minibatch만큼 돌려서 batch_size만큼 train_X와 train_T 배열을 슬라이싱 한 후 각각 forward와 backward, gradient descent를 시킨다. \n",
    "#int형 랜덤변수 seed값을 받아서 해당 seed값으로 data와 label에 동일하게 셔플을 진행시킨다. 즉 setseed라는 값으로 seed값을 변수로 저장시키고 그 값으로 shuffle을 진행해야 train과 label의 순서가 바뀌지 않는다. \n",
    "        miniSize = np.arange(0,train_X.shape[0], batch_size)\n",
    "        setseed = np.random.randint(0,10000)\n",
    "        np.random.seed(setseed)\n",
    "        train_X = np.random.shuffle(train_X)\n",
    "        np.random.seed(setseed)\n",
    "        train_T = np.random.shuffle(train_T)\n",
    "        \n",
    "        for j in miniSize:\n",
    "            mini_X = dataset['train_X'][j: j + batch_size,:]\n",
    "            mini_T = dataset['one_hot_train'][j: j + batch_size,:]\n",
    "    \n",
    "            Loss = ThreeLayerNet.forward(mini_X, mini_T)\n",
    "            back = ThreeLayerNet.backpropagation(mini_X, mini_T)\n",
    "            ThreeLayerNet.gradientdescent(back, learning_rate)\n",
    "        \n",
    "        if (i % 10 == 0) and ((i//10) < 3 or ( i//10) >8):\n",
    "            train_acc = ThreeLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'])\n",
    "            test_acc = ThreeLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'])\n",
    "            print(i, '\\t번째 Loss = ', Loss)\n",
    "            print(i, '\\t번째 Train_Accuracy : ', train_acc)\n",
    "            print(i, '\\t번째 Test_Accuracy : ', test_acc)\n",
    "            #train_acc_list.append(train_acc)\n",
    "            #test_acc_list.append(test_acc)\n",
    "            #Loss_list.append(Loss)  \n",
    "\n",
    "    return ThreeLayerNet #, train_acc_list, test_acc_list, Loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_use_Optimizer(dataset, ThreeLayerNet, learning_rate, epoch, kill_n_h1 = 0.25, kill_n_h2 = 0.15):\n",
    "#ThreeLayerNet.killRate함수를 이용하면 해당 클래스 내의 self.flag 값이 1로 setting이 된다. 이는 ThreeLayerNet에서 forward연산 진행 시 flag=1이므로 dropout을 사용하는 if문의 함수가 실행되어 dropout 방식을 사용하게 된다. \n",
    "#그렇기 때문에 현재 함수에서는 killRate를 설정하는 것 외에는 기존의 batch optimizer방식과 동일하다.\n",
    "\n",
    "    ThreeLayerNet.killRate(kill_n_h1, kill_n_h2) #threeLayerNet에 kill rate를 세팅한다. \n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    Loss_list = []     \n",
    "    for i in range(epoch+1):\n",
    "        #코드 작성\n",
    "        Loss = ThreeLayerNet.forward(dataset['train_X'], dataset['one_hot_train'])\n",
    "        back = ThreeLayerNet.backpropagation(dataset['train_X'], dataset['one_hot_train'])\n",
    "        ThreeLayerNet.gradientdescent(back, learning_rate)\n",
    "    \n",
    "    \n",
    "        if (i % 10 == 0) and ((i//10) < 3 or ( i//10) >97):\n",
    "            train_acc = ThreeLayerNet.accuracy(dataset['train_X'], dataset['one_hot_train'])\n",
    "            test_acc = ThreeLayerNet.accuracy(dataset['test_X'], dataset['one_hot_test'])\n",
    "            print(i, '\\t번째 Loss = ', Loss)\n",
    "            print(i, '\\t번째 Train_Accuracy : ', train_acc)\n",
    "            print(i, '\\t번째 Test_Accuracy : ', test_acc)\n",
    "            train_acc_list.append(train_acc)\n",
    "            test_acc_list.append(test_acc)\n",
    "            Loss_list.append(Loss)  \n",
    "    return ThreeLayerNet, train_acc_list, test_acc_list, Loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#과제 채점을 위한 세팅\n",
    "train_X, train_label, test_X, test_label = train_test_split(mnist)\n",
    "\n",
    "one_hot_train = one_hot_encoding(train_label)\n",
    "one_hot_test = one_hot_encoding(test_label)\n",
    "\n",
    "dataset = {}\n",
    "dataset['train_X'] = train_X\n",
    "dataset['test_X'] = test_X\n",
    "dataset['one_hot_train'] = one_hot_train\n",
    "dataset['one_hot_test'] = one_hot_test\n",
    "\n",
    "neournlist = [784, 60, 30, 10]\n",
    "\n",
    "TNN_batchOptimizer = ThreeLayerNet(neournlist)\n",
    "TNN_minibatchOptimizer = copy.deepcopy(TNN_batchOptimizer)\n",
    "TNN_dropout = copy.deepcopy(TNN_minibatchOptimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \t번째 Loss =  2.3451291573205766\n",
      "0 \t번째 Train_Accuracy :  0.116125\n",
      "0 \t번째 Test_Accuracy :  0.106\n",
      "10 \t번째 Loss =  2.0228148836517645\n",
      "10 \t번째 Train_Accuracy :  0.36725\n",
      "10 \t번째 Test_Accuracy :  0.3415\n",
      "20 \t번째 Loss =  1.665647540592542\n",
      "20 \t번째 Train_Accuracy :  0.523625\n",
      "20 \t번째 Test_Accuracy :  0.5445\n",
      "980 \t번째 Loss =  0.11197481309169213\n",
      "980 \t번째 Train_Accuracy :  0.97075\n",
      "980 \t번째 Test_Accuracy :  0.9435\n",
      "990 \t번째 Loss =  0.11065985230680978\n",
      "990 \t번째 Train_Accuracy :  0.97125\n",
      "990 \t번째 Test_Accuracy :  0.9435\n",
      "1000 \t번째 Loss =  0.10935877304893304\n",
      "1000 \t번째 Train_Accuracy :  0.972125\n",
      "1000 \t번째 Test_Accuracy :  0.9435\n",
      "0 \t번째 Loss =  0.5514946845409198\n",
      "0 \t번째 Train_Accuracy :  0.85475\n",
      "0 \t번째 Test_Accuracy :  0.881\n",
      "10 \t번째 Loss =  0.16847799155477605\n",
      "10 \t번째 Train_Accuracy :  0.963125\n",
      "10 \t번째 Test_Accuracy :  0.941\n",
      "20 \t번째 Loss =  0.057413791444092616\n",
      "20 \t번째 Train_Accuracy :  0.98875\n",
      "20 \t번째 Test_Accuracy :  0.952\n",
      "90 \t번째 Loss =  0.0019376940474464662\n",
      "90 \t번째 Train_Accuracy :  1.0\n",
      "90 \t번째 Test_Accuracy :  0.955\n",
      "100 \t번째 Loss =  0.0017024691511692764\n",
      "100 \t번째 Train_Accuracy :  1.0\n",
      "100 \t번째 Test_Accuracy :  0.955\n",
      "0 \t번째 Loss =  2.3281095021260563\n",
      "0 \t번째 Train_Accuracy :  0.113625\n",
      "0 \t번째 Test_Accuracy :  0.101\n",
      "10 \t번째 Loss =  2.1287884905295646\n",
      "10 \t번째 Train_Accuracy :  0.32775\n",
      "10 \t번째 Test_Accuracy :  0.2975\n",
      "20 \t번째 Loss =  1.9056158942282118\n",
      "20 \t번째 Train_Accuracy :  0.44175\n",
      "20 \t번째 Test_Accuracy :  0.4425\n",
      "980 \t번째 Loss =  0.35702623954457924\n",
      "980 \t번째 Train_Accuracy :  0.937625\n",
      "980 \t번째 Test_Accuracy :  0.941\n",
      "990 \t번째 Loss =  0.3581305778133579\n",
      "990 \t번째 Train_Accuracy :  0.938\n",
      "990 \t번째 Test_Accuracy :  0.9415\n",
      "1000 \t번째 Loss =  0.3613380741618025\n",
      "1000 \t번째 Train_Accuracy :  0.938125\n",
      "1000 \t번째 Test_Accuracy :  0.941\n"
     ]
    }
   ],
   "source": [
    "#채점은 이 것의 결과값으로 할 예정입니다. \n",
    "\n",
    "trained_batch, tb_train_acc_list, tb_test_acc_list, tb_loss_list =  batchOptimization(dataset, TNN_batchOptimizer, 0.1, 1000)\n",
    "trained_minibatch = minibatch_Optimization(dataset, TNN_minibatchOptimizer, 0.1, epoch=100, batch_size=100)\n",
    "trained_dropout, td_train_acc_list, td_test_acc_list, td_loss_list = dropout_use_Optimizer(dataset, TNN_dropout, 0.1, 1000, 0.25, 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
